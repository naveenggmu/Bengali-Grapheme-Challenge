{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResnetYourself.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SPJWETdIguFg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"1a831078-c8a2-48d2-904d-a5ce526631e8","executionInfo":{"status":"ok","timestamp":1578073742756,"user_tz":-330,"elapsed":81303,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjKmPF9djFeX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":326},"outputId":"8c13c604-418f-4833-c2a8-5b5909355e23","executionInfo":{"status":"ok","timestamp":1578074144249,"user_tz":-330,"elapsed":7918,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["pip install keras-resnet"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting keras-resnet\n","  Downloading https://files.pythonhosted.org/packages/76/d4/a35cbd07381139dda4db42c81b88c59254faac026109022727b45b31bcad/keras-resnet-0.2.0.tar.gz\n","Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet) (2.2.5)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (1.17.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (1.3.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet) (2.8.0)\n","Building wheels for collected packages: keras-resnet\n","  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20485 sha256=1c3ac78a145ef64accdd964f81f3db2878318ab787f3c19bda47c7bab4c21cbc\n","  Stored in directory: /root/.cache/pip/wheels/5f/09/a5/497a30fd9ad9964e98a1254d1e164bcd1b8a5eda36197ecb3c\n","Successfully built keras-resnet\n","Installing collected packages: keras-resnet\n","Successfully installed keras-resnet-0.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j0zUlx4sjFWN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eq11trQ0jwNi","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import h5py\n","import math\n","\n","def load_dataset():\n","    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n","\n","\n","def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n","    mini_batch_size - size of the mini-batches, integer\n","    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n","    \n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","    \n","    m = X.shape[0]                  # number of training examples\n","    mini_batches = []\n","    np.random.seed(seed)\n","    \n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[permutation,:,:,:]\n","    shuffled_Y = Y[permutation,:]\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n","        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n","        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    return mini_batches\n","\n","\n","def convert_to_one_hot(Y, C):\n","    Y = np.eye(C)[Y.reshape(-1)].T\n","    return Y\n","\n","\n","def forward_propagation_for_predict(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n","    \n","    Arguments:\n","    X -- input dataset placeholder, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n","                  the shapes are given in initialize_parameters\n","    Returns:\n","    Z3 -- the output of the last LINEAR unit\n","    \"\"\"\n","    \n","    # Retrieve the parameters from the dictionary \"parameters\" \n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    W3 = parameters['W3']\n","    b3 = parameters['b3'] \n","                                                           # Numpy Equivalents:\n","    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n","    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n","    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n","    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n","    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n","    \n","    return Z3\n","\n","def predict(X, parameters):\n","    \n","    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n","    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n","    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n","    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n","    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n","    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n","    \n","    params = {\"W1\": W1,\n","              \"b1\": b1,\n","              \"W2\": W2,\n","              \"b2\": b2,\n","              \"W3\": W3,\n","              \"b3\": b3}\n","    \n","    x = tf.placeholder(\"float\", [12288, 1])\n","    \n","    z3 = forward_propagation_for_predict(x, params)\n","    p = tf.argmax(z3)\n","    \n","    sess = tf.Session()\n","    prediction = sess.run(p, feed_dict = {x: X})\n","        \n","    return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LZ_CtVghQNC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"outputId":"da8c726f-66f4-4f9f-aa8f-66b91e7890ef","executionInfo":{"status":"ok","timestamp":1578075299244,"user_tz":-330,"elapsed":3141,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["\n","import numpy as np\n","from keras import layers\n","from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n","from keras.models import Model, load_model\n","from keras.preprocessing import image\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from keras.applications.imagenet_utils import preprocess_input\n","import pydot\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.utils import plot_model\n","# from resnets_utils import *\n","from keras.initializers import glorot_uniform\n","import scipy.misc\n","from matplotlib.pyplot import imshow\n","%matplotlib inline\n","\n","import keras.backend as K\n","K.set_image_data_format('channels_last')\n","K.set_learning_phase(1)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"naOzncITi1K0","colab_type":"code","colab":{}},"source":["def identity_block(X, f, filters, stage, block):\n","    \"\"\"\n","    Implementation of the identity block as defined in Figure 3\n","    \n","    Arguments:\n","    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    f -- integer, specifying the shape of the middle CONV's window for the main path\n","    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n","    stage -- integer, used to name the layers, depending on their position in the network\n","    block -- string/character, used to name the layers, depending on their position in the network\n","    \n","    Returns:\n","    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n","    \"\"\"\n","    \n","    # defining name basis\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # Retrieve Filters\n","    F1, F2, F3 = filters\n","    \n","    # Save the input value. You'll need this later to add back to the main path. \n","    X_shortcut = X\n","    \n","    # First component of main path\n","    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","    X = Activation('relu')(X)\n","\n","    \n","    # Second component of main path (≈3 lines)\n","    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n","    X = Activation('relu')(X)\n","\n","    # Third component of main path (≈2 lines)\n","    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n","    X = Add()([X, X_shortcut])\n","    X = Activation('relu')(X)\n","    \n","    \n","    return X"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULwv366bkc7z","colab_type":"code","colab":{}},"source":["def convolutional_block(X, f, filters, stage, block, s = 2):\n","    \"\"\"\n","    Implementation of the convolutional block as defined in Figure 4\n","    \n","    Arguments:\n","    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    f -- integer, specifying the shape of the middle CONV's window for the main path\n","    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n","    stage -- integer, used to name the layers, depending on their position in the network\n","    block -- string/character, used to name the layers, depending on their position in the network\n","    s -- Integer, specifying the stride to be used\n","    \n","    Returns:\n","    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n","    \"\"\"\n","    \n","    # defining name basis\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # Retrieve Filters\n","    F1, F2, F3 = filters\n","    \n","    # Save the input value\n","    X_shortcut = X\n","\n","\n","    ##### MAIN PATH #####\n","    # First component of main path \n","    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","    X = Activation('relu')(X)\n","\n","    # Second component of main path (≈3 lines)\n","    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n","    X = Activation('relu')(X)\n","\n","\n","    # Third component of main path (≈2 lines)\n","    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n","\n","\n","    ##### SHORTCUT PATH #### (≈2 lines)\n","    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n","                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n","    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n","    X = Add()([X, X_shortcut])\n","    X = Activation('relu')(X)\n","    \n","    \n","    return X\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9muBHKkJkiUX","colab_type":"code","colab":{}},"source":["def ResNet50(input_shape=(64, 64, 3), classes=6):\n","    \"\"\"\n","    Implementation of the popular ResNet50 the following architecture:\n","    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n","    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n","\n","    Arguments:\n","    input_shape -- shape of the images of the dataset\n","    classes -- integer, number of classes\n","\n","    Returns:\n","    model -- a Model() instance in Keras\n","    \"\"\"\n","\n","    # Define the input as a tensor with shape input_shape\n","    X_input = Input(input_shape)\n","\n","    # Zero-Padding\n","    X = ZeroPadding2D((3, 3))(X_input)\n","\n","    # Stage 1\n","    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n","\n","    # Stage 2\n","    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n","    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n","    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n","\n","    ### START CODE HERE ###\n","\n","    # Stage 3 (≈4 lines)\n","    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n","    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n","    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n","    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n","\n","    # Stage 4 (≈6 lines)\n","    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n","\n","    # Stage 5 (≈3 lines)\n","    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n","    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n","    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n","\n","    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n","    X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n","\n","    ### END CODE HERE ###\n","\n","    # output layer\n","    X = Flatten()(X)\n","    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n","    \n","    \n","    # Create model\n","    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNfMDH78krfB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":411},"outputId":"93aa374b-b325-4892-e7dd-f18e8cccffaf","executionInfo":{"status":"ok","timestamp":1578074744079,"user_tz":-330,"elapsed":5560,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["model = ResNet50(input_shape = (137, 236, 1), classes = 7)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jk29UIOAk3nC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"3ff4d522-4586-4cf1-acb0-4ee37b564f1a","executionInfo":{"status":"ok","timestamp":1578074748386,"user_tz":-330,"elapsed":864,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QCZmfTL4k4Nh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"61416d15-f94b-44bf-d0da-da3e725dfc4f","executionInfo":{"status":"ok","timestamp":1578075683716,"user_tz":-330,"elapsed":90792,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["#Data Splitting\n","import numpy as np\n","import os\n","os.chdir('/content/gdrive/My Drive/Bengali/train_npy/')\n","X_train0 = np.load('train_0.npy')\n","X_train1 = np.load('train_1.npy')\n","X_train2 = np.load('train_2.npy')\n","# X_train3 = np.load('train_3.npy')\n","print(X_train0.shape)\n","print(X_train1.shape)\n","print(X_train2.shape)\n","# print(X_train3.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["(50210, 137, 236)\n","(50210, 137, 236)\n","(50210, 137, 236)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGamcq97k4dI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"53504214-39ff-4331-fe32-e1d5ae6cf306","executionInfo":{"status":"ok","timestamp":1578075804098,"user_tz":-330,"elapsed":7669,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["X_train0 = np.concatenate((X_train0,X_train1,X_train2))\n","print(X_train0.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(150630, 137, 236)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"32uC4fvok4j_","colab_type":"code","colab":{}},"source":["np.save('/content/gdrive/My Drive/Bengali/train_npy/train_till_2.npy',X_train0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7rWLWdyk4q9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIi539AQk5N2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"deda56ee-81c7-4154-ecba-fe0fec942769","executionInfo":{"status":"ok","timestamp":1578076059351,"user_tz":-330,"elapsed":80618,"user":{"displayName":"Naveen Narayanan","photoUrl":"","userId":"14894238330134112731"}}},"source":["#Data Splitting\n","import numpy as np\n","import os\n","os.chdir('/content/gdrive/My Drive/Bengali/train_npy/')\n","X_train0 = np.load('train_till_2.npy')\n","X_train3 = np.load('train_3.npy')\n","# X_train2 = np.load('train_2.npy')\n","# X_train3 = np.load('train_3.npy')\n","print(X_train0.shape)\n","print(X_train3.shape)\n","# print(X_train2.shape)\n","# print(X_train3.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["(150630, 137, 236)\n","(50210, 137, 236)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZJ5-rUAcqHtX","colab_type":"code","colab":{}},"source":["X_train0 = np.concatenate((X_train0,X_train3))\n","print(X_train0.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gc6gQV9wqNvR","colab_type":"code","colab":{}},"source":["np.save('/content/gdrive/My Drive/Bengali/train_npy/train_till_2.npy',X_train0)"],"execution_count":0,"outputs":[]}]}